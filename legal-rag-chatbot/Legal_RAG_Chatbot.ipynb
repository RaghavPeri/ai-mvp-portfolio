{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIw580UrAlN46xSsb7Nn64",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaghavPeri/ai-mvp-portfolio/blob/main/legal-rag-chatbot/Legal_RAG_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“„ Legal RAG Chatbot (LangChain + Google Sheets MVP)\n",
        "\n",
        "This notebook demonstrates a minimal yet scalable **Retrieval-Augmented Generation (RAG)** pipeline using **LangChain**, **OpenAI**, and **Google Sheets**.  \n",
        "It allows legal questions to be entered into a shared Google Sheet, where answers are automatically generated by retrieving relevant chunks from a legal PDF and sending them to a GPT model for contextual response.\n",
        "\n",
        "### âœ… What It Covers:\n",
        "- PDF ingestion and chunking using LangChain\n",
        "- Vector embedding using OpenAI\n",
        "- Semantic retrieval via FAISS\n",
        "- LLM-powered response generation\n",
        "- Google Sheets integration for input/output\n",
        "- Conditional row-level automation using `RunnableSequence`\n",
        "\n",
        "This MVP is optimized for extension into a real-time app or API by swapping out the sheet logic with a form or frontend trigger.\n"
      ],
      "metadata": {
        "id": "tVGZSkdxfUnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”§  Setup and LangChain RAG Pipeline Initialization\n",
        "\n",
        "This block:\n",
        "1. Installs all required dependencies (`langchain`, `langchain-community`, `langchain-openai`, etc.)\n",
        "2. Loads the uploaded legal document (PDF)\n",
        "3. Splits the document into overlapping chunks\n",
        "4. Creates vector embeddings using OpenAI\n",
        "5. Stores vectors in a FAISS index for fast retrieval\n",
        "6. Assembles the LangChain `RetrievalQA` pipeline using `OpenAI` LLM\n",
        "\n",
        "ğŸ“Œ This cell is run once per session to prepare the RAG engine (`qa_chain`) before processing any user questions.\n"
      ],
      "metadata": {
        "id": "MoqRBGhwf34H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiFJAueE_mqv",
        "outputId": "e613f7f9-4cc1-42cb-fb58-53a98c6a3518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/64.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/723.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m723.4/723.4 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/438.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m438.5/438.5 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hTotal chunks created: 55\n",
            "âœ… Step 6 Complete: RAG-powered QA chain successfully built and ready to answer questions.\n"
          ]
        }
      ],
      "source": [
        "## Step 1: Install LangChain + LangChain-OpenAI (official LLM & embedding classes)\n",
        "!pip install -q -U langchain langchain-community langchain-openai faiss-cpu openai tiktoken pypdf\n",
        "\n",
        "## Step 2: Import Required Libraries\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
        "import os\n",
        "\n",
        "## Step 3:  Upload a Legal PDF\n",
        "## Use Colabâ€™s left panel â†’ Files â†’ Click Upload.\n",
        "## Upload a public legal document, e.g., â€œTenant Rights.pdfâ€\n",
        "\n",
        "## Step 4: Load the legal PDF\n",
        "loader = PyPDFLoader(\"Tenant Rights.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "# Split into manageable chunks\n",
        "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(f\"Total chunks created: {len(chunks)}\")\n",
        "\n",
        "## Step 5:Embed & Create FAISS Vector Store\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"  # ğŸ”’ Replace before running\n",
        "\n",
        "\n",
        "# Create vector embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "## Step 6: Build RAG-powered QA Chain\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(temperature=0),\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")\n",
        "print(\"âœ… Step 6 Complete: RAG-powered QA chain successfully built and ready to answer questions.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§  RunnableSequence + Google Sheets Integration\n",
        "\n",
        "This block:\n",
        "1. Authenticates Google Sheets access via `gspread`\n",
        "2. Reads questions, answers, and timestamps from the sheet\n",
        "3. Finds the latest unanswered question\n",
        "4. Uses a LangChain `RunnableSequence` to:\n",
        "   - Check if a question needs to be answered\n",
        "   - Run the `qa_chain` if needed\n",
        "   - Write the answer and a timestamp (in local time) to the sheet\n",
        "5. Handles errors gracefully by logging them into a dedicated column\n",
        "\n",
        "ğŸ“Œ This design ensures the system only processes new questions, leaving prior responses untouched.\n"
      ],
      "metadata": {
        "id": "pEzzznj3fYhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 1: Install & Import\n",
        "!pip install -q gspread oauth2client\n",
        "import gspread\n",
        "from google.auth import default\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
        "\n",
        "## Step 2: Authenticate and connect to Google Sheets\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "## Step 3: Connect to your Google Sheet\n",
        "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1_QmYN7ac26U5mzW-fbgmxCKTeOEd9MebsfAO6GIS_Uk/edit\"\n",
        "sheet = gc.open_by_url(spreadsheet_url).sheet1\n",
        "\n",
        "## Step 4: Fetch all sheet data\n",
        "# Skip the header (first row)\n",
        "questions  = sheet.col_values(1)[1:]\n",
        "answers    = sheet.col_values(2)[1:]\n",
        "timestamps = sheet.col_values(3)[1:]\n",
        "\n",
        "num_rows = len(questions)\n",
        "latest_index = None\n",
        "\n",
        "# Step 5: Identify the latest new question (with no answer or timestamp)\n",
        "for i in reversed(range(num_rows)):\n",
        "    q = questions[i].strip()\n",
        "    a = answers[i].strip() if i < len(answers) else \"\"\n",
        "    t = timestamps[i].strip() if i < len(timestamps) else \"\"\n",
        "\n",
        "    if q and not a and not t:\n",
        "        latest_index = i\n",
        "        break\n",
        "\n",
        "## Step 6: Define LangChain RunnableSequence for clean chaining\n",
        "\n",
        "def check_if_question_needs_answer(row_dict):\n",
        "    if row_dict[\"question\"] and not row_dict[\"answer\"] and not row_dict[\"timestamp\"]:\n",
        "        return row_dict[\"question\"]\n",
        "    return None  # Skip\n",
        "\n",
        "def generate_answer(question):\n",
        "    return {\n",
        "        \"answer\": qa_chain.run(question),\n",
        "        \"timestamp\": datetime.now(pytz.timezone(\"America/New_York\")).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    }\n",
        "\n",
        "# Define LangChain Runnable pipeline\n",
        "check = RunnableLambda(check_if_question_needs_answer)\n",
        "rag_chain = RunnableLambda(generate_answer)\n",
        "pipeline = RunnableSequence(check | rag_chain)\n",
        "\n",
        "## Step 7: Run pipeline only if there is a new question\n",
        "if latest_index is not None:\n",
        "    latest_q = questions[latest_index]\n",
        "    row_data = {\n",
        "        \"question\": latest_q,\n",
        "        \"answer\": answers[latest_index] if latest_index < len(answers) else \"\",\n",
        "        \"timestamp\": timestamps[latest_index] if latest_index < len(timestamps) else \"\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        result = pipeline.invoke(row_data)\n",
        "        sheet.update_cell(latest_index + 2, 2, result[\"answer\"])       # Column B\n",
        "        sheet.update_cell(latest_index + 2, 3, result[\"timestamp\"])    # Column C\n",
        "\n",
        "        print(\"âœ… Latest Legal Question Answered:\")\n",
        "        print(f\"\\nğŸŸ¨ Question: {latest_q}\\nğŸŸ© Answer: {result['answer']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        sheet.update_cell(latest_index + 2, 4, f\"Error: {str(e)}\")     # Column D for errors\n",
        "        print(f\"âŒ Error: {str(e)}\")\n",
        "else:\n",
        "    print(\"âœ… No new unanswered questions found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxpIQB9OI1cd",
        "outputId": "6cae0b75-b69c-4324-cb25-dc72ec929caa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Latest Legal Question Answered:\n",
            "\n",
            "ğŸŸ¨ Question: who makes repairs inside the unit if theres a damage ?\n",
            "ğŸŸ© Answer:  According to the context provided, it is the responsibility of the landlord to make repairs inside the unit if there is damage. The tenant must first notify the landlord of the situation and allow a reasonable amount of time for the landlord to make repairs or replacements. If the landlord fails to take action, the tenant may have the repairs made and deduct the cost from future rent. However, the tenant may still be taken to court for nonpayment of rent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… MVP Summary: Legal RAG Chatbot with Google Sheets Integration  \n",
        "This MVP answers user-submitted legal questions by applying Retrieval-Augmented Generation (RAG) on a legal PDF. The entire flow is orchestrated using LangChain and responses are written back to a Google Sheet.\n",
        "\n",
        "### ğŸ” What It Does:\n",
        "- Loads and chunks a legal document (e.g., *Tenant Rights*) using LangChain utilities  \n",
        "- Creates vector embeddings and stores them in a FAISS index  \n",
        "- Detects newly added questions in a Google Sheet  \n",
        "- Retrieves relevant PDF chunks and generates contextual answers using an LLM  \n",
        "- Writes the answer and timestamp back to the sheet without modifying prior entries\n",
        "\n",
        "### ğŸ§° Tech Stack:\n",
        "- Google Colab + Google Sheets API (`gspread`)  \n",
        "- LangChain + `langchain-openai`  \n",
        "- OpenAI Embeddings + GPT-based LLM  \n",
        "- FAISS for semantic document retrieval  \n",
        "- Python `RunnableSequence` for modular flow control\n",
        "\n",
        "### ğŸ¯ Product Thinking:\n",
        "- Enables legal teams or citizens to get contextual document-backed answers in a familiar spreadsheet format  \n",
        "- Mimics customer service or self-serve Q&A tools without needing a full app  \n",
        "- Designed for low-friction upgrades â€” e.g., swapping the sheet trigger for a web form or API\n"
      ],
      "metadata": {
        "id": "D-9PSGa2gsrm"
      }
    }
  ]
}