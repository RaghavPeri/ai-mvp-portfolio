{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAa+uMRXUnaOO0H7oVUAxB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaghavPeri/ai-mvp-portfolio/blob/main/resume-analyzer/Resume_Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧾 Resume Analyzer – Modes Overview\n",
        "\n",
        "This notebook includes two modes to test resume-to-JD semantic fit:\n",
        "\n",
        "- **Google Docs integration** — for real-world use, automation, and demo storytelling  \n",
        "- **Static input (manual paste)** — for quick testing and prototyping  \n",
        "\n",
        "Skip to your preferred version using the section headers below.\n"
      ],
      "metadata": {
        "id": "GT074UgBB5BH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resume Analyzer MVP (with Google Docs Integration)\n",
        "This notebook pulls a resume and job description directly from Google Docs, generates text embeddings using OpenAI, and calculates their similarity.\n"
      ],
      "metadata": {
        "id": "NiIV74iB4yUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 1: This is to Mount Google Drive\n",
        "# This command connects your Google Drive to the Colab notebook,\n",
        "# so we can access Google Docs and files directly from your account.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5n359zO5IT3",
        "outputId": "cf800f0b-34da-4198-ee73-525e75ef401e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: This is to Install Required Libraries\n",
        "# These packages let us interact with the Google Docs API to pull resume and JD text,\n",
        "# and use the OpenAI API to generate embeddings.\n",
        "!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HbOFsxcr6J5t",
        "outputId": "6fd07461-2900-4604-e83a-d837e56b9650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.170.0)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.82.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 3: This is to Authenticate Google Account\n",
        "# This step allows Colab to access your Google Docs so we can fetch resume and JD content.\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ],
      "metadata": {
        "id": "JCoygwwo7WH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 4: Define Function to Read Google Docs Content\n",
        "# This function takes a Google Doc ID, connects to the Docs API,\n",
        "# and extracts all the plain text content (used for both resume and JD).\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "import google.auth\n",
        "\n",
        "# Build the Docs service using default credentials\n",
        "creds, _ = google.auth.default()\n",
        "docs_service = build('docs', 'v1', credentials=creds)\n",
        "\n",
        "def get_doc_content(doc_id):\n",
        "    doc = docs_service.documents().get(documentId=doc_id).execute()\n",
        "    text = ''\n",
        "    for content in doc.get('body').get('content'):\n",
        "        if 'paragraph' in content:\n",
        "            for el in content['paragraph'].get('elements', []):\n",
        "                if 'textRun' in el:\n",
        "                    text += el['textRun'].get('content', '')\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "ILIx-QBr73Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 5: Load Resume and Job Description from Google Docs\n",
        "# Replace the document IDs below with your actual Google Doc IDs\n",
        "# (found in the URL between /d/ and /edit)\n",
        "\n",
        "resume_doc_id = \"1y7FNZx-3COkRc2BFHpmM8bZtn68NI2qyRR3Z8xzkdjs\"\n",
        "jd_doc_id = \"1vzwDrxWsxsxz92w56Ds1GNKOBhSfwgbf0uRTIFaKDv4\"\n",
        "\n",
        "# Pull the actual text content from both documents\n",
        "resume_text = get_doc_content(resume_doc_id)\n",
        "jd_text = get_doc_content(jd_doc_id)\n",
        "\n",
        "# Preview the first few lines of each (optional)\n",
        "print(\"Resume preview:\\n\", resume_text[:300])\n",
        "print(\"\\nJD preview:\\n\", jd_text[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DT6gBxth8a-O",
        "outputId": "47ce991e-4e2c-4cfb-f4ce-9eed0f89e59e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resume preview:\n",
            " RAGHAV PERI\n",
            "Senior Technical Project/Product Manager | AI-Powered Product Innovation| Retail & Fintech\n",
            "Piscataway, NJ | +1 (980) 833-5649 | raghavperis@gmail.com |  linkedin.com/in/raghavperi\n",
            " \n",
            "Professional Summary\n",
            "Senior Technical Project/Product Manager with close to 17 years of experience deliver\n",
            "\n",
            "JD preview:\n",
            " \n",
            "Public\n",
            "Product Manager\n",
            "\n",
            "About the job\u000bPublic is an investing platform that makes building a multi-asset portfolio fast, frictionless, and secure. Members can trade stocks, options, bonds, crypto, ETFs, and alternative assets—all in one place. Alongside its robust suite of investing tools, Public of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 6: Generate Embeddings and Display Similarity Result\n",
        "# This block compares the resume and JD using OpenAI embeddings and cosine similarity.\n",
        "\n",
        "import openai\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "openai.api_key = \"YOUR_API_KEY\" # Replace with your OpenAI API key\n",
        "\n",
        "# Define the embedding function OpenAI v1+\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "    response = openai.embeddings.create(\n",
        "        input=[text],\n",
        "        model=model\n",
        "    )\n",
        "    return np.array(response.data[0].embedding)\n",
        "\n",
        "# Generate embeddings for resume and job description\n",
        "resume_vec = get_embedding(resume_text)\n",
        "jd_vec = get_embedding(jd_text)\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity_score = cosine_similarity([resume_vec], [jd_vec])[0][0]\n",
        "print(f\"🔍 Similarity Score: {round(similarity_score * 100, 2)}%\")\n",
        "\n",
        "# Show match interpretation\n",
        "if similarity_score < 0.7:\n",
        "    print(\"❌ Resume does not match the job well.\")\n",
        "else:\n",
        "    print(\"✅ Resume is a strong match for the job.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9gbgBU3Cyjl",
        "outputId": "0727831e-8284-4549-8ed5-8c5e770073da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Similarity Score: 78.47%\n",
            "✅ Resume is a strong match for the job.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Alternate Version: Static Resume + JD (Manual Entry)\n",
        "Use this version to test locally by pasting the resume and job description directly below.\n",
        "\n",
        "This approach skips Google Docs and is useful for quick prototyping or fallback mode.\n",
        "\n",
        "```python\n",
        "resume_text = \"\"\"Paste your resume here\"\"\"\n",
        "jd_text = \"\"\"Paste your job description here\"\"\"\n"
      ],
      "metadata": {
        "id": "0eBcNujUA3B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 1: Install and Import Required Libraries\n",
        "# We'll use OpenAI for embeddings, numpy for vector math, and sklearn for cosine similarity\n",
        "\n",
        "!pip install openai\n",
        "import openai\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MAiSYzSJIiJX",
        "outputId": "ff81100f-c064-4f8e-ff9a-bc8ea8462733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.82.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEQqzL65eaOK",
        "outputId": "c51abcf1-84e1-4a91-d826-bacc3bfe83c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity Score: 79.27%\n",
            "✅ Resume is a strong match for the job.\n"
          ]
        }
      ],
      "source": [
        "## Step 2: Set Your OpenAI API Key\n",
        "# You’ll need an OpenAI account and API key. This allows access to text-embedding-ada-002\n",
        "\n",
        "openai.api_key = \"YOUR_API_KEY\"  # Replace this with your actual key\n",
        "\n",
        "## Step 3: Paste Resume and Job Description Text\n",
        "# Copy text from your Google Docs and paste here as plain strings\n",
        "\n",
        "resume_text = \"\"\"RAGHAV PERI\n",
        "Senior Technical Project/Product Manager | AI-Powered Product Innovation| Retail & Fintech\n",
        "Piscataway, NJ | +1 (980) 833-5649 | raghavperis@gmail.com |  linkedin.com/in/raghavperi\n",
        "\n",
        "Professional Summary\n",
        "Senior Technical Project/Product Manager with close to 17 years of experience delivering AI-powered, user-centric platforms across retail, supply chain, and banking. Adept at designing enterprise architecture, leading system integration (IBM OMS, SAP S/4HANA, SFCC, CAR, CRM), and embedding AI into product lifecycles to drive performance, personalization, and automation. Built scalable frameworks rolled out across global brands like Coach, Kate Spade, and Stuart Weitzman — optimizing order-to-cash, fulfillment, and post-order services.\n",
        "Delivered multimillion-dollar value across digital payments, reconciliation, and CSR tooling through strategic innovation, agile execution, and full-lifecycle product ownership. Recently developed 3 end-to-end AI MVPs to solve critical DTC personalization problems using supervised ML, LLM APIs, and rapid prototyping with real business impact.\n",
        "Outcomes delivered & Notable achievements\n",
        "Unlocked $48M in incremental revenue by launching Apple Pay, Shop Pay, and BNPL across digital checkout, reducing cart abandonment by 22% and improving transaction success by 15%.\n",
        "Recovered $1.2M in missed revenue by leading SAP–OMS reconciliation remediation across three global brands, cutting financial exceptions by 50%.\n",
        "Created $2.5M in annual value impact by enabling real-time visibility into order-to-cash workflows, reducing leakage, manual tracking, and SLA misses.\n",
        "Reduced refund cycle time by 35% and improved reconciliation accuracy by 30% by embedding post-order logic and returns into SAP workflows.\n",
        "Built 3 AI MVPs — Feedback Classifier (NLP), Fit & Match Predictor (Supervised ML), and Churn Intelligence Layer (Random Forest) — using lean, no-code/low-code tech stacks (Google Sheets, Forms, Colab, App Script, OpenAI API). Each MVP was delivered in under 8 hours with clear customer value and scalable potential.\n",
        "Directed and produced a feature film that qualified for the 2018 Academy Awards, showcasing creative leadership, storytelling, and end-to-end execution under resource constraints.\n",
        "Designed and published a portfolio of product strategy case studies across AI, fintech, and retail — including scroll-to-sale TikTok commerce, GenAI copilots, and DTC personalization systems. Link to Portfolio: bit.ly/portfolio-raghavperi\n",
        "Core Competencies\n",
        "Product Strategy & Roadmap Development\n",
        "AI/ML Integration & GenAI Solutions\n",
        "Customer-Centric Feature Development\n",
        "Agile & Scrum Methodologies\n",
        "Data-Driven Decision Making\n",
        "Cloud & Digital Transformation\n",
        "Business Process Optimization\n",
        "Full-Lifecycle Product Ownership\n",
        "Tools & Technologies\n",
        "Enterprise Platforms: IBM Sterling OMS, SAP S/4HANA, SFCC, Salesforce, CAR\n",
        "Cloud & Infra: AWS, Azure, Unix/Linux\n",
        "Data & AI: Informatica, SQL, Oracle, Netezza, Greenplum, OpenAI API, Google Colab\n",
        "Agile Stack: JIRA, Confluence, Rally, Aha!, ProductBoard, Miro, Google App Script\n",
        " Professional Experience\n",
        "Tapestry, NJ\tJun 2018 to Present\n",
        "Product Manager- IBM Sterling OMS & SAP S/4 HANA\n",
        "Launched Apple Pay, Shop Pay, and BNPL, reducing cart abandonment by 22%, improving transaction success by 15%, and unlocking $48M in incremental revenue within 6 months.\n",
        "Defined and executed a fulfillment-focused product roadmap and PRD cycles, aligning engineering and business teams on feature scope, GTM priorities, and time-to-market decisions — resulting in a 20% increase in order processing speed and a 15% boost in customer satisfaction.\n",
        "Integrated IBM Sterling OMS with SAP S/4HANA financial workflows, improving monthly financial close accuracy by 30% and reducing manual reconciliation effort by 40%.\n",
        "Led a $1M SAP finance remediation initiative, resolving reconciliation mismatches across three global brands, recovering $1.2M in missed revenue, and reducing exceptions by 50%.\n",
        "Enhanced refund logic by embedding return workflows into SAP, reducing refund cycle time by 35% and improving SLA adherence and customer satisfaction.\n",
        "Designed and implemented Repairs and Appeasements modules, enabling CSR-triggered credit/debit memos and reason-code-driven GL mapping between OMS and SAP — reducing CSR handling time by 40% and improving post-order resolution accuracy.\n",
        "Enabled $2.5M in annual value by building real-time visibility into order-to-cash workflows, minimizing revenue leakage and manual exception handling.\n",
        "Refined orchestration logic using A/B testing, analytics, and customer journey insights, achieving a 20% reduction in order cycle time and 30% scalability improvement during peak sales periods.\n",
        "Applied GenAI (post-MIT training) to automate customer review classification, reducing manual tagging effort by 35% and accelerating product feedback loops.\n",
        "Scaled core OMS and integration architecture across Coach, Kate Spade, and Stuart Weitzman, reducing onboarding time for new brand rollouts by 40% and ensuring regional consistency across SFCC, SAP, and CRM platforms.\n",
        "Orchestrated Agile transformation across tech and business teams during SAP transition — achieving 80%+ sprint throughput and reducing stabilization time by 30% through structured backlog management and triage protocols.\n",
        "TIAA CREF, NJ\tFeb 2017 to Jun 2018\n",
        "Product Manager - Digital Metrics\n",
        "Built a digital metrics platform to track user drop-offs and engagement across retirement and investment journeys, resulting in a 25% increase in digital engagement and improved transaction visibility.\n",
        "Defined and operationalized KPIs like Click-to-Goal (CTG) and digitization score, enabling UX prioritization that boosted self-service adoption by 20%.\n",
        "Identified campaign drop-offs among elderly users, collaborating with marketing and analytics to deploy personalized robo-advisor messaging — increasing click-through and re-engagement by 12%.\n",
        "Implemented QA automation frameworks for tracking product performance, reducing manual effort by 50% and accelerating release cycles.\n",
        "Partnered with engineering and analytics to embed data-driven decision-making into the product lifecycle, laying the groundwork for future AI/ML integration.\n",
        "JP Morgan & Chase, DE\tJun 2014 to Jan 2017\n",
        "Product Owner/Manager - Risk MIS & Portfolio Dashboards\n",
        "Delivered real-time risk dashboards for trust and estate portfolio managers, enabling data-informed AUM allocation and ensuring 100% compliance with audit and regulatory standards.\n",
        "Led a global Agile team of 20+ across multiple time zones, driving consistent sprint execution, backlog alignment, and stakeholder communication.\n",
        "Developed digital transformation tools for risk, cost, and portfolio performance analytics, including AI/ML-based financial data workflows across retirement, annuity, and investment platforms.\n",
        "Optimized ETL pipelines, reducing data latency by 35%, enabling near real-time reporting and faster risk-based decisions.\n",
        "Prioritized dashboard enhancements based on business value, enabling executive-level revenue forecasting and performance tracking.\n",
        "Bank of America, NC\tJul 2013 to Jun 2014\n",
        "Technical Business Analyst/ETL Informatica Tech Lead - FAST Data Mart\n",
        "Built an Oracle-based Data Mart to support 360° client visibility across business functions.\n",
        "Led a team of 8 offshore developers to deliver real-time ETL pipelines and ensure SLA-bound production stability.\n",
        "Resolved Informatica bottlenecks, improving load performance and nightly job completion time.\n",
        "Fidelity, India\tNov 2012 to Jun 2013\n",
        "ETL Informatica Technology Lead\n",
        "Led migration of 2,800+ ETL objects and 6,000+ Unix scripts, supporting 24+ applications.\n",
        "Trained and mentored the Dalian, China team, enabling stable, repeatable post-migration execution.\n",
        "Wells Fargo Advisors, India\tFeb 2009 to Nov 2012 ETL Informatica Tech Lead / Lead Developer\n",
        "Designed and deployed ETL frameworks for ODS, Data Marts, and Data Warehouses, supporting compliance and fund reporting.\n",
        "Tuned performance of Informatica workflows and conducted code reviews for release readiness.\n",
        "AVID, India\tJun 2008 to Feb 2009\n",
        "System Engineer\n",
        "Developed BMP encoder/decoder codec for AVID’s media platform, improving cross-format image handling.\n",
        "Researched and authored 20-page spec for RED camera’s R3D format, enabling early AVID support during digital cinema transition.\n",
        " Education Details and Certifications\n",
        "Bachelor of Technology in Computer Science & Technology from JNTU, India in 2008.\n",
        "Product Management Certification, Cornell University (Dec 2023)\n",
        "No-Code AI & ML: Building Data Science Solutions, MIT Professional Education (Nov 2024)\n",
        "SAFe Product Owner/Product Manager (POPM) — Training Completed, Certification in Progress (Expected May 2025)\"\"\"\n",
        "\n",
        "jd_text = \"\"\"\n",
        "Public\n",
        "Product Manager\n",
        "\n",
        "About the job\n",
        "Public is an investing platform that makes building a multi-asset portfolio fast, frictionless, and secure. Members can trade stocks, options, bonds, crypto, ETFs, and alternative assets—all in one place. Alongside its robust suite of investing tools, Public offers Alpha, a layer of artificial intelligence that provides fundamental data and custom analysis to support informed investment decisions.\n",
        "\n",
        "Since 2019, Public has raised over $300 million. Investors include Accel, Tiger Global, Will Smith's Dreamers VC, The Chainsmokers' Mantis VC, and Shari Redstone's Advancit Capital, as well as renowned figures in business and culture, like Maria Sharapova, Tony Hawk, and NYU Stern professor Scott Galloway.\n",
        "\n",
        "What You’ll Do\n",
        "\n",
        "We are looking for a highly-motivated Product Manager to join our nimble product team. In this role, you’ll:\n",
        "\n",
        "\n",
        "\n",
        "Drive end-to-end execution of new investing products and tools from ideation, to product positioning & goals, design & iteration, ticketing & refinements, development, QA, and launch.\n",
        "Collaborate closely with engineering, design, leadership, design, marketing, customer support, and compliance experts to build the primary investing account for the next generation.\n",
        "Be the first point of contact to unblock whatever engineering needs to be as effective as possible at their job.\n",
        "Have a strong pulse on the overall market and our current and potential new customers to understand the current investing landscape, sentiment, and customer needs to scale, grow, iterate, and differentiate Public’s business.\n",
        "Develop initial go to market hypothesis on what we’d have to believe for the product to be a success, including the target audience, goals (KPIs/impact), product positioning/messaging, and key differentiators\n",
        "Be involved in the development of Public’s entire product roadmap, balancing speed, quality and impact to meet the most important needs of our customers\n",
        "\n",
        "\n",
        "This role is based in New York City, and you will be required to work from our office at minimum, Monday through Thursday.\n",
        "\n",
        "Who You Are\n",
        "\n",
        "You have direct experience working at a brokerage, working in financial or investing markets, working with investment products, or are extremely passionate and active in the financial markets\n",
        "4+ years of experience in product, management consulting, or related industry experience within a startup environment building high-quality consumer or financial products; a minimum of 1 year of experience directly in product\n",
        "Self-starter attitude and ability to execute the entire product lifecycle, from ideation to execution, analysis, and continuous iteration\n",
        "Excellent verbal and written communication skills to work cross-functionally with teams of engineers, compliance, risk, operations, go-to-market and more\n",
        "\n",
        "\n",
        "Bonus Points\n",
        "\n",
        "Experience leading API integrations with external partners\n",
        "Awareness of U.S. financial regulations\n",
        "\n",
        "\n",
        "Public is an equal employment opportunity employer to all employees and applicants for employment and prohibits discrimination and harassment of any type. We celebrate people of all race, color, religion, age, sex, national origin, disability status, genetics, veteran status, sexual orientation, gender identity, or expression. The compensation range for this role is $150-$180k based on skills and experience.\n",
        "\"\"\"\n",
        "\n",
        "## Step 4: Define a Function to Generate Embeddings\n",
        "# This sends text to OpenAI’s API and returns a 1536-dimensional vector\n",
        "# Define the embedding function OpenAI v1+\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "    response = openai.embeddings.create(\n",
        "        input=[text],\n",
        "        model=model\n",
        "    )\n",
        "    return np.array(response.data[0].embedding)\n",
        "\n",
        "## Step 5: Convert Resume and JD Text to Vectors\n",
        "# We now embed both documents so we can compare them mathematically\n",
        "\n",
        "resume_vec = get_embedding(resume_text)\n",
        "jd_vec = get_embedding(jd_text)\n",
        "\n",
        "## Step 6: Calculate Similarity Using Cosine Distance\n",
        "# This gives a similarity score between 0 (not similar) and 1 (very similar)\n",
        "\n",
        "similarity_score = cosine_similarity([resume_vec], [jd_vec])[0][0]\n",
        "print(f\"Similarity Score: {round(similarity_score * 100, 2)}%\")\n",
        "\n",
        "## Step 7: Output a Simple Match Assessment\n",
        "# Based on the similarity score, print whether the resume matches the job well\n",
        "\n",
        "if similarity_score < 0.7:\n",
        "    print(\"❌ Resume does not match the job well.\")\n",
        "else:\n",
        "    print(\"✅ Resume is a strong match for the job.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ MVP Summary: Resume Analyzer with Google Docs Integration\n",
        "\n",
        "This MVP analyzes how well a resume matches a job description using AI-powered text embeddings and cosine similarity.\n",
        "\n",
        "### 🔍 What It Does:\n",
        "- Pulls resume and JD content directly from Google Docs (no manual copy-paste)\n",
        "- Converts both texts into vector embeddings using OpenAI's `text-embedding-ada-002`\n",
        "- Calculates semantic similarity between resume and JD\n",
        "- Flags whether the match is strong or weak based on a 70% similarity threshold\n",
        "\n",
        "### 🧰 Tech Stack:\n",
        "- Google Colab + Google Docs API\n",
        "- OpenAI Embeddings (`text-embedding-ada-002`)\n",
        "- NumPy + scikit-learn (cosine similarity)\n",
        "\n",
        "### 🎯 Product Thinking:\n",
        "- Built for demo-friendliness: dynamic inputs, minimal friction\n",
        "- Mirrors a real-world PM need — quick resume-to-JD matching at scale\n",
        "- Frames resume match strength as a proxy for product-fit (candidate-to-role relevance)\n",
        "- Can be repurposed as a semantic search use case in recruiting, ATS, or GenAI-based job platforms\n",
        "- Supports both static/manual mode and Docs-integrated mode\n",
        "\n"
      ],
      "metadata": {
        "id": "tBzEIqxlE9HU"
      }
    }
  ]
}